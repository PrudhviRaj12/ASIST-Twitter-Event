{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from tweepy import API\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import Cursor\n",
    "import json\n",
    "\n",
    "consumer_key = #your consumer key here\n",
    "consumer_secret = #your consumer secret here\n",
    "\n",
    "access_key = #your access key here\n",
    "access_secret = #your access secret here\n",
    "\n",
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_key, access_secret)\n",
    "\n",
    "client = API(auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Home Timeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for status in Cursor(client.home_timeline).items(20):\n",
    "    print (status.text)\n",
    "    print (\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hash Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for status in Cursor(client.search, q = \"#MeToo\").items(20):\n",
    "    print (status.text)\n",
    "    print (\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for status in Cursor(client.search, q = 'Deep Learning').items(20):\n",
    "    print (status.text)\n",
    "    print (\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect Tweets For Further Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_list = []\n",
    "c = 0 #counter\n",
    "for status in Cursor(client.search, q = \"#Metoo\").items(200):\n",
    "    c+=1\n",
    "    print (c)\n",
    "    tweets_list.append(status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweet Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#https://developer.twitter.com/en/docs/tweets/data-dictionary/overview/tweet-object\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect the texts from the tweet data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets_text = []\n",
    "for t in tweets_list:\n",
    "    tweets_text.append(t.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Word Frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count_dict = {}\n",
    "for text in tweets_text:\n",
    "    for word in text.split():\n",
    "        if word not in word_count_dict:\n",
    "            word_count_dict[word] = 1\n",
    "        else:\n",
    "            word_count_dict[word] = word_count_dict[word] + 1\n",
    "\n",
    "print (\"Size of the vocabulary: \" + str(len(word_count_dict)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorting Word Frequencies to Find most frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "sorted_frequencies = sorted(word_count_dict.items(), key = operator.itemgetter(1), reverse = True)\n",
    "print (sorted_frequencies[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "document_frequency = {}\n",
    "for term in word_count_dict:\n",
    "    occurence_count = 0\n",
    "    for text in tweets_text:\n",
    "        if term in text:\n",
    "            occurence_count +=1\n",
    "    document_frequency[term] = occurence_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorting the Document Frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_doc_frequencies = sorted(document_frequency.items(), key = operator.itemgetter(1), reverse = True)\n",
    "print (sorted_doc_frequencies[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Term Frequency - Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_scores = {}\n",
    "vocabulary = word_count_dict.keys()\n",
    "\n",
    "for word in vocabulary:\n",
    "    tfidf_scores[word]  = word_count_dict[word]/ (document_frequency[word] + 0.00001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorting the TFIDF Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_tfidf_scores = sorted(tfidf_scores.items(), key = operator.itemgetter(1), reverse = True)\n",
    "print (sorted_tfidf_scores[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Clouds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Simple Word Cloud\n",
    "\n",
    "```\n",
    "Installing wordcloud\n",
    "\n",
    "1. download: http://www.lfd.uci.edu/~gohlke/pythonlibs/#wordcloud\n",
    "\n",
    "2. cd to the file path\n",
    "\n",
    "3. Run this command python -m pip install <filename>\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wordcloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#magic line for inline plots\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "input_text = ' '\n",
    "for texts in tweets_text:\n",
    "    for words in texts.split():\n",
    "            input_text = input_text + ' ' + words\n",
    "\n",
    "wc = wordcloud.WordCloud().generate(input_text)\n",
    "plt.imshow(wc, interpolation = 'bilinear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A \"little\" preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_word = 'https'\n",
    "input_text = ' '\n",
    "for texts in tweets_text:\n",
    "    for words in texts.split():\n",
    "        if check_word not in words:\n",
    "            input_text = input_text + ' ' + words\n",
    "\n",
    "wc = wordcloud.WordCloud().generate(input_text)\n",
    "plt.imshow(wc, interpolation = 'bilinear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One More Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_word_1 = 'https'\n",
    "check_word_2 = 'MeToo'\n",
    "\n",
    "input_text = ' '\n",
    "for texts in tweets_text:\n",
    "    for words in texts.split():\n",
    "        if check_word_1 not in words and check_word_2 not in words:\n",
    "            input_text = input_text + ' ' + words\n",
    "            \n",
    "wc = wordcloud.WordCloud().generate(input_text)\n",
    "plt.imshow(wc, interpolation = 'bilinear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This idea can be generalized by creating a list of terms we don't want and \n",
    "checking their existance at every iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traffic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.dates as mdates\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a list with the dates and times of tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_dates = []\n",
    "for dates in tweets_list:\n",
    "    tweets_dates.append(dates.created_at)\n",
    "print (\"Sample Format: \" + str(tweets_dates[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Time Series for the Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_indexing = pd.DatetimeIndex(tweets_dates)\n",
    "dummy_value = np.ones(len(tweets_dates))\n",
    "series = pd.Series(dummy_value, index = sample_indexing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregating Tweets within a certain interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sampling_time = '1Min'\n",
    "aggregator = series.resample(sampling_time).sum().fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the tweet frequencies as a function of time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "hours = mdates.MinuteLocator(interval= 1)\n",
    "date_formatter = mdates.DateFormatter('%H:%M')\n",
    "\n",
    "datemin = min(tweets_dates)#datetime(2017, 10, 14, 18, 15)\n",
    "datemax = max(tweets_dates)#datetime(2017, 10, 14, 19, 00)\n",
    "\n",
    "ax.xaxis.set_major_locator(hours)\n",
    "ax.xaxis.set_major_formatter(date_formatter)\n",
    "ax.set_xlim(datemin, datemax)\n",
    "max_freq = aggregator.max()\n",
    "#ax.set_ylim(0, max_freq)\n",
    "ax.plot(aggregator.index, aggregator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Locating the peaks in the plot and move to next step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "peaklow = datetime(2017, 10, 21, 12, 50)\n",
    "peakhigh = datetime(2017, 10, 21, 12, 51)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving the tweet that generated massive traffic in that time frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ci = 0 # Counter\n",
    "retweet_count = []\n",
    "filtered_tweets = []\n",
    "for tweets in tweets_list:\n",
    "    if tweets.created_at >= peaklow and  tweets.created_at < peakhigh :\n",
    "        ci +=1\n",
    "        print (ci)\n",
    "        retweet_count.append(tweets.retweet_count)\n",
    "        filtered_tweets.append(tweets.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Maximum Number of Retweets: \" +str(max(retweet_count)))\n",
    "print (\"Index of the Tweet with Maximum Number of Retweets: \" +str(np.argmax(retweet_count)))\n",
    "print (\"Tweet that generated maximum traffic: \\n \") \n",
    "print (filtered_tweets[np.argmax(retweet_count)]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Global Activity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting Longitudes and Latitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "longitude = []\n",
    "latitude= []\n",
    "\n",
    "for tweets in tweets_list:\n",
    "    if tweets.coordinates:\n",
    "        longitude.append(tweets.coordinates['coordinates'][0])\n",
    "        latitude.append(tweets.coordinates['coordinates'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.basemap import Basemap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "m =  Basemap(projection='merc',llcrnrlat=-80,urcrnrlat=80,\\\n",
    "            llcrnrlon=-180,urcrnrlon=180,lat_ts=20,resolution='c')\n",
    "m.drawcoastlines()\n",
    "m.fillcontinents(color='white',lake_color='aqua')\n",
    "m.drawparallels(np.arange(-90.,91.,30.))\n",
    "m.drawmeridians(np.arange(-180.,181.,60.))\n",
    "m.drawmapboundary(fill_color='aqua')\n",
    "xpt,ypt = m(longitude,latitude)\n",
    "lonpt, latpt = m(xpt,ypt,inverse=True)\n",
    "m.plot(xpt,ypt,'bo', color = 'red')  # plot a blue dot there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
