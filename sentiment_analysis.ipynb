{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "#os.chdir('Twit')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Importing Data File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'encoding' is an invalid keyword argument for this function",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-4e61ba4500a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'training.txt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"utf8\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'encoding' is an invalid keyword argument for this function"
     ]
    }
   ],
   "source": [
    "#data = open('training.txt', 'r',encoding=\"utf8\").read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = open('training.txt', 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Structuring Data to Further Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets = []\n",
    "targets = []\n",
    "for d in data:\n",
    "    tweets.append(d[2:])\n",
    "    targets.append(int(d[0]))\n",
    "\n",
    "tweets = np.array(tweets)\n",
    "targets = np.array(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Selector to seperate some examples (1000)\n",
    "chosen at random for testing (these will not be used for training)\n",
    "\n",
    "Since we are doing this pretty naively, there might be a duplicate\n",
    "examples in the test set, but for now, we can ignore that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random_selector = np.random.randint(0, len(data), 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Selecting Test Data based on the random indices generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = tweets[random_selector]\n",
    "test_targets = targets[random_selector]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Collecting Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_data = []\n",
    "training_targets = []\n",
    "\n",
    "for d in range(0, len(data)):\n",
    "    if d not in random_selector:\n",
    "        training_data.append(data[d][2:])\n",
    "        training_targets.append(int(data[d][0]))\n",
    "\n",
    "training_data = np.array(training_data)\n",
    "training_targets = np.array(training_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Counting words across all the tweets, irrespective of their class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_count = dict()\n",
    "for tweet in training_data:\n",
    "    for word in tweet.split():\n",
    "        if word not in word_count:\n",
    "            word_count[word] = 1\n",
    "        else:\n",
    "            word_count[word] +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Calculating the probability of a word occuring in a tweet, irrespective of the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_prob = dict()\n",
    "total_words = sum(word_count.values()) + 0.0\n",
    "\n",
    "for word in word_count:\n",
    "    word_prob[word] = word_count[word]/total_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Seperating Positive and Negative Classes Positive = 1, Negative = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "positive_class = np.where(training_targets == 1)[0]\n",
    "negative_class = np.where(training_targets == 0)[0]\n",
    "\n",
    "positive_tweets = training_data[positive_class]\n",
    "negative_tweets = training_data[negative_class]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Word Occurences in Positive Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "positive_class_count = {}\n",
    "for tweet in positive_tweets:\n",
    "    for word in tweet.split():\n",
    "        if word not in positive_class_count:\n",
    "            positive_class_count[word] = 1\n",
    "        else:\n",
    "            positive_class_count[word] +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Word Occurences in Negative Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "negative_class_count = {}\n",
    "for tweet in negative_tweets:\n",
    "    for word in tweet.split():\n",
    "        if word not in negative_class_count:\n",
    "            negative_class_count[word] = 1\n",
    "        else:\n",
    "            negative_class_count[word] +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###To maintain balance to reduce errors while testing, for every word that is in the entire vocabulary, but not in seperate class vocabulary, we add the word that class vocabulary and give it a count of zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for word in word_prob:\n",
    "    if word not in positive_class_count:\n",
    "        positive_class_count[word] = 0\n",
    "    if word not in negative_class_count:\n",
    "        negative_class_count[word] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Small Value to avoid divide by zero error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epsilon = 0.00001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Conditional Probabilites of a word occuring given the tweet is positive or negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "positive_class_prob, negative_class_prob = {}, {}    \n",
    "for word in word_prob:\n",
    "    summer = positive_class_count[word] + negative_class_count[word]\n",
    "    positive_class_prob[word] = (positive_class_count[word]/summer) +epsilon\n",
    "    negative_class_prob[word] = (negative_class_count[word]/summer) +epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Test Sample\n",
    "\n",
    "To avoid long multiplications that would eventually\n",
    "combine to zero, we use logarithm of the value.\n",
    "\n",
    "log (a x b) = log(a) + log(b), which converts multiplication\n",
    "into addition, thereby reducing the possiblity of the value extending to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('I am going to start reading the Harry Potter series again because that is one awesome story.', 1)\n",
      "Positive\n"
     ]
    }
   ],
   "source": [
    "sample = test_data[0]\n",
    "target_needed = test_targets[0]\n",
    "\n",
    "positive = 0\n",
    "negative = 0\n",
    "for word in sample.split():\n",
    "    if word in positive_class_prob:\n",
    "        positive += (np.log(word_prob[word]) + np.log(positive_class_prob[word]))\n",
    "    if word in negative_class_prob:\n",
    "        negative += (np.log(word_prob[word]) + np.log(negative_class_prob[word]))\n",
    "        \n",
    "predicted_sentiment = int(positive > negative)\n",
    "print (sample, target_needed)\n",
    "if predicted_sentiment == 1:\n",
    "    print (\"Positive\")\n",
    "else:\n",
    "    print (\"Negative\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Finding the accuracy of our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Correct Predictions: 858\n",
      "Accuracy: 0.858\n"
     ]
    }
   ],
   "source": [
    "correct_predictions = 0\n",
    "for t in range(0, len(test_data)):    \n",
    "    sample = test_data[t]\n",
    "    target_needed = test_targets[t]\n",
    "    \n",
    "    positive = 0\n",
    "    negative = 0\n",
    "    for word in sample.split():\n",
    "        if word in positive_class_prob:\n",
    "            positive += (np.log(word_prob[word]) + np.log(positive_class_prob[word]))\n",
    "        if word in negative_class_prob:\n",
    "            negative += (np.log(word_prob[word]) + np.log(negative_class_prob[word]))\n",
    "            \n",
    "    predicted_sentiment = int(positive > negative)\n",
    "    \n",
    "    if predicted_sentiment == target_needed:\n",
    "        correct_predictions += 1\n",
    "        \n",
    "print (\"Number of Correct Predictions: \" + str(correct_predictions))\n",
    "print (\"Accuracy: \" + str(correct_predictions/(len(test_data) + 0.0)))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
